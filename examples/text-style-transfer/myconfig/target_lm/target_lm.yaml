model_name: meta-llama/Llama-2-7b-chat-hf
batch_size: 8
torch_dtype: bf16
template: "{input}{prompt}"

generation_configs:
  do_sample: True
  max_new_tokens: 10
  num_return_sequences: 1
  top_p: 1
  top_k: 10
